{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym\n",
    "\n",
    "## Predefined Environments\n",
    "\n",
    "OpenAI Gym offers several predefined environment to train RL agents.\n",
    "The code for each environment group is housed in its own subdirectory\n",
    "[gym/envs](https://github.com/openai/gym/blob/master/gym/envs). The\n",
    "specification of each task is in\n",
    "[gym/envs/\\_\\_init\\_\\_.py](https://github.com/openai/gym/blob/master/gym/envs/__init__.py).\n",
    "It's worth browsing through both.\n",
    "\n",
    "### Algorithmic\n",
    "\n",
    "These are a variety of algorithmic tasks, such as learning to copy a\n",
    "sequence.\n",
    "\n",
    "``` python\n",
    "import gym\n",
    "env = gym.make('Copy-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "```\n",
    "\n",
    "### Atari\n",
    "\n",
    "The Atari environments are a variety of Atari video games. If you didn't\n",
    "do the full install, you can install dependencies via `pip install -e\n",
    "'.[atari]'` (you'll need `cmake` installed) and then get started as\n",
    "follows:\n",
    "\n",
    "``` python\n",
    "import gym\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "```\n",
    "\n",
    "This will install `atari-py`, which automatically compiles the [Arcade\n",
    "Learning Environment](http://www.arcadelearningenvironment.org/). This\n",
    "can take quite a while (a few minutes on a decent laptop), so just be\n",
    "prepared.\n",
    "\n",
    "### Box2d\n",
    "\n",
    "Box2d is a 2D physics engine. You can install it via `pip install -e\n",
    "'.[box2d]'` and then get started as follows:\n",
    "\n",
    "``` python\n",
    "import gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "env.render()\n",
    "```\n",
    "\n",
    "### Classic control\n",
    "\n",
    "These are a variety of classic control tasks, which would appear in a\n",
    "typical reinforcement learning textbook. If you didn't do the full\n",
    "install, you will need to run `pip install -e '.[classic_control]'` to\n",
    "enable rendering. You can get started with them via:\n",
    "\n",
    "``` python\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "```\n",
    "\n",
    "### MuJoCo\n",
    "\n",
    "[MuJoCo](http://www.mujoco.org/) is a physics engine which can do very\n",
    "detailed efficient simulations with contacts. It's not open-source, so\n",
    "you'll have to follow the instructions in\n",
    "[mujoco-py](https://github.com/openai/mujoco-py#obtaining-the-binaries-and-license-key)\n",
    "to set it up. You'll have to also run `pip install -e '.[mujoco]'` if\n",
    "you didn't do the full install.\n",
    "\n",
    "``` python\n",
    "import gym\n",
    "env = gym.make('Humanoid-v2')\n",
    "env.reset()\n",
    "env.render()\n",
    "```\n",
    "\n",
    "### Robotics\n",
    "\n",
    "These environments also use [MuJoCo](http://www.mujoco.org/). You'll have to also run `pip install -e '.[robotics]'` if\n",
    "you didn't do the full install.\n",
    "\n",
    "``` python\n",
    "import gym\n",
    "env = gym.make('HandManipulateBlock-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "```\n",
    "\n",
    "You can also find additional details in the accompanying [technical\n",
    "report](https://arxiv.org/abs/1802.09464) and [blog\n",
    "post](https://blog.openai.com/ingredients-for-robotics-research/). If\n",
    "you use these environments, you can cite them as follows:\n",
    "\n",
    "    @misc{1802.09464,\n",
    "      Author = {Matthias Plappert and Marcin Andrychowicz and Alex Ray and Bob McGrew and Bowen Baker and Glenn Powell and Jonas Schneider and Josh Tobin and Maciek Chociej and Peter Welinder and Vikash Kumar and Wojciech Zaremba},\n",
    "      Title = {Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research},\n",
    "      Year = {2018},\n",
    "      Eprint = {arXiv:1802.09464},\n",
    "    }\n",
    "\n",
    "### Toy text\n",
    "\n",
    "Toy environments which are text-based. There's no extra dependency to\n",
    "install, so to get started, you can just do:\n",
    "\n",
    "``` python\n",
    "import gym\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "```\n",
    "\n",
    "## OpenAI Environments\n",
    "\n",
    "### Procgen\n",
    "\n",
    "16 simple-to-use procedurally-generated gym environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills. The environments run at high speed (thousands of steps per second) on a single core.\n",
    "\n",
    "Learn more here: https://github.com/openai/procgen\n",
    "\n",
    "### Gym-Retro\n",
    "\n",
    "Gym Retro lets you turn classic video games into Gym environments for reinforcement learning and comes with integrations for ~1000 games. It uses various emulators that support the Libretro API, making it fairly easy to add new emulators.\n",
    "\n",
    "Learn more here: https://github.com/openai/retro\n",
    "\n",
    "### Roboschool (DEPRECATED)\n",
    "\n",
    "**We recommend using the [PyBullet Robotics Environments](#pybullet-robotics-environments) instead**\n",
    "\n",
    "3D physics environments like Mujoco environments but uses the Bullet physics engine and does not require a commercial license.\n",
    "\n",
    "Learn more here: https://github.com/openai/roboschool\n",
    "\n",
    "## Third Party Environments\n",
    "\n",
    "The gym comes prepackaged with many many environments. It's this common API around many environments that makes Gym so great. Here we will list additional environments that do not come prepacked with the gym. Submit another to this list via a pull-request.\n",
    "\n",
    "### PyBullet Robotics Environments\n",
    "\n",
    "3D physics environments like the Mujoco environments but uses the Bullet physics engine and does not require a commercial license.  Works on Mac/Linux/Windows.\n",
    "\n",
    "Learn more here: https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.wz5to0x8kqmr\n",
    "\n",
    "### Obstacle Tower\n",
    "\n",
    "3D procedurally generated tower where you have to climb to the highest level possible\n",
    "\n",
    "Learn more here: https://github.com/Unity-Technologies/obstacle-tower-env\n",
    "\n",
    "Platforms: Windows, Mac, Linux\n",
    "\n",
    "### PGE: Parallel Game Engine\n",
    "\n",
    "PGE is a FOSS 3D engine for AI simulations, and can interoperate with the Gym. Contains environments with modern 3D graphics, and uses Bullet for physics.\n",
    "\n",
    "Learn more here: https://github.com/222464/PGE\n",
    "\n",
    "### gym-inventory: Inventory Control Environments\n",
    "\n",
    "gym-inventory is a single agent domain featuring discrete state and action spaces that an AI agent might encounter in inventory control problems. \n",
    "\n",
    "Learn more here: https://github.com/paulhendricks/gym-inventory\n",
    "\n",
    "### gym-gazebo: training Robots in Gazebo\n",
    "\n",
    "gym-gazebo presents an extension of the initial OpenAI gym for robotics using ROS and Gazebo, an advanced 3D modeling and\n",
    "rendering  tool.\n",
    "\n",
    "Learn more here: https://github.com/erlerobot/gym-gazebo/\n",
    "\n",
    "### gym-maze: 2D maze environment\n",
    "A simple 2D maze environment where an agent finds its way from the start position to the goal. \n",
    "\n",
    "Learn more here: https://github.com/tuzzer/gym-maze/\n",
    "\n",
    "### osim-rl: Musculoskeletal Models in OpenSim\n",
    "\n",
    "A human musculoskeletal model and a physics-based simulation environment where you can synthesize physically and physiologically accurate motion. One of the environments built in this framework is a competition environment for a NIPS 2017 challenge.\n",
    "\n",
    "Learn more here: https://github.com/stanfordnmbl/osim-rl\n",
    "\n",
    "### gym-minigrid: Minimalistic Gridworld Environment\n",
    "\n",
    "A minimalistic gridworld environment. Seeks to minimize software dependencies, be easy to extend and deliver good performance for faster training.\n",
    "\n",
    "Learn more here: https://github.com/maximecb/gym-minigrid\n",
    "\n",
    "### gym-miniworld: Minimalistic 3D Interior Environment Simulator \n",
    "\n",
    "MiniWorld is a minimalistic 3D interior environment simulator for reinforcement learning & robotics research. It can be used to simulate environments with rooms, doors, hallways and various objects (eg: office and home environments, mazes). MiniWorld can be seen as an alternative to VizDoom or DMLab. It is written 100% in Python and designed to be easily modified or extended.\n",
    "\n",
    "Learn more here: https://github.com/maximecb/gym-miniworld\n",
    "\n",
    "### gym-sokoban: 2D Transportation Puzzles\n",
    "\n",
    "The environment consists of transportation puzzles in which the player's goal is to push all boxes on the warehouse's storage locations.\n",
    "The advantage of the environment is that it generates a new random level every time it is initialized or reset, which prevents over fitting to predefined levels.\n",
    "\n",
    "Learn more here: https://github.com/mpSchrader/gym-sokoban\n",
    "\n",
    "### gym-duckietown: Lane-Following Simulator for Duckietown\n",
    "\n",
    "A lane-following simulator built for the [Duckietown](http://duckietown.org/) project (small-scale self-driving car course).\n",
    "\n",
    "Learn more here: https://github.com/duckietown/gym-duckietown\n",
    "\n",
    "### GymFC: A flight control tuning and training framework \n",
    "\n",
    "GymFC is a modular framework for synthesizing neuro-flight controllers. The\n",
    "architecture integrates digital twinning concepts to provide seamless transfer\n",
    "of trained policies to hardware. The OpenAI environment has been used to\n",
    "generate policies for the worlds first open source neural network flight\n",
    "control firmware [Neuroflight](https://github.com/wil3/neuroflight).\n",
    "\n",
    "Learn more here: https://github.com/wil3/gymfc/\n",
    "\n",
    "### gym-anytrading: Environments for trading markets\n",
    "\n",
    "AnyTrading is a collection of OpenAI Gym environments for reinforcement learning-based trading algorithms with a great focus on simplicity, flexibility, and comprehensiveness.\n",
    "\n",
    "Learn more here: https://github.com/AminHP/gym-anytrading\n",
    "\n",
    "### GymGo: The Board Game Go\n",
    "\n",
    "An implementation of the board game Go\n",
    "\n",
    "Learn more here: https://github.com/aigagror/GymGo \n",
    "\n",
    "### gym-electric-motor: Intelligent control of electric drives\n",
    "\n",
    "An environment for simulating a wide variety of electric drives taking into account different types of electric motors and converters. Control schemes can be continuous, yielding a voltage duty cycle, or discrete, determining converter switching states directly.\n",
    "\n",
    "Learn more here: https://github.com/upb-lea/gym-electric-motor\n",
    "\n",
    "### NASGym: gym environment for Neural Architecture Search (NAS)\n",
    "\n",
    "The environment is fully-compatible with the OpenAI baselines and exposes a NAS environment following the Neural Structure Code of [BlockQNN: Efficient Block-wise Neural Network Architecture Generation](https://arxiv.org/abs/1808.05584). Under this setting, a Neural Network (i.e. the state for the reinforcement learning agent) is modeled as a list of NSCs, an action is the addition of a layer to the network, and the reward is the accuracy after the early-stop training. The datasets considered so far are the CIFAR-10 dataset (available by default) and the meta-dataset (has to be manually downloaded as specified in [this repository](https://github.com/gomerudo/meta-dataset)).\n",
    "\n",
    "Learn more here: https://github.com/gomerudo/nas-env\n",
    "\n",
    "### gym-jiminy: training Robots in Jiminy\n",
    "\n",
    "gym-jiminy presents an extension of the initial OpenAI gym for robotics using Jiminy, an extremely fast and light weight simulator for poly-articulated systems using Pinocchio for physics evaluation and Meshcat for web-based 3D rendering.\n",
    "\n",
    "Learn more here: https://github.com/Wandercraft/jiminy\n",
    "\n",
    "### highway-env: Tactical Decision-Making for Autonomous Driving\n",
    "\n",
    "An environment for behavioural planning in autonomous driving, with an emphasis on high-level perception and decision rather than low-level sensing and control. The difficulty of the task lies in understanding the social interactions with other drivers, whose behaviours are uncertain. Several scenes are proposed, such as highway, merge, intersection and roundabout.\n",
    "\n",
    "Learn more here: https://github.com/eleurent/highway-env\n",
    "\n",
    "## Custom environments\n",
    "\n",
    "### How to create new environments for Gym\n",
    "\n",
    "* Create a new repo called gym-foo, which should also be a PIP package.\n",
    "\n",
    "* A good example is https://github.com/openai/gym-soccer.\n",
    "\n",
    "* It should have at least the following files:\n",
    "  ```sh\n",
    "  gym-foo/\n",
    "    README.md\n",
    "    setup.py\n",
    "    gym_foo/\n",
    "      __init__.py\n",
    "      envs/\n",
    "        __init__.py\n",
    "        foo_env.py\n",
    "        foo_extrahard_env.py\n",
    "  ```\n",
    "\n",
    "* `gym-foo/setup.py` should have:\n",
    "\n",
    "  ```python\n",
    "  from setuptools import setup\n",
    "\n",
    "  setup(name='gym_foo',\n",
    "        version='0.0.1',\n",
    "        install_requires=['gym']  # And any other dependencies foo needs\n",
    "  )\n",
    "  ```\n",
    "\n",
    "* `gym-foo/gym_foo/__init__.py` should have:\n",
    "  ```python\n",
    "  from gym.envs.registration import register\n",
    "\n",
    "  register(\n",
    "      id='foo-v0',\n",
    "      entry_point='gym_foo.envs:FooEnv',\n",
    "  )\n",
    "  register(\n",
    "      id='foo-extrahard-v0',\n",
    "      entry_point='gym_foo.envs:FooExtraHardEnv',\n",
    "  )\n",
    "  ```\n",
    "\n",
    "* `gym-foo/gym_foo/envs/__init__.py` should have:\n",
    "  ```python\n",
    "  from gym_foo.envs.foo_env import FooEnv\n",
    "  from gym_foo.envs.foo_extrahard_env import FooExtraHardEnv\n",
    "  ```\n",
    "\n",
    "* `gym-foo/gym_foo/envs/foo_env.py` should look something like:\n",
    "  ```python\n",
    "  import gym\n",
    "  from gym import error, spaces, utils\n",
    "  from gym.utils import seeding\n",
    "\n",
    "  class FooEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "      ...\n",
    "    def step(self, action):\n",
    "      ...\n",
    "    def reset(self):\n",
    "      ...\n",
    "    def render(self, mode='human'):\n",
    "      ...\n",
    "    def close(self):\n",
    "      ...\n",
    "  ```\n",
    "\n",
    "* After you have installed your package with `pip install -e gym-foo`, you can create an instance of the environment with `gym.make('gym_foo:foo-v0')`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "For more information see\n",
    "- https://www.gymlibrary.dev/content/environment_creation/ for custom envs\n",
    "- https://www.gymlibrary.dev/ for predefined envs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
